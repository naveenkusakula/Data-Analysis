from pyspark import SparkContext
logFile = "file:////home/ubuntu/data/taskdcleantweets.txt"
sc = SparkContext("local", "first app")
text_file = sc.textFile(logFile)
counts = text_file.flatMap(lambda line: line.split("\n"))
rd = counts.filter(lambda x: "not safe" in x)
result = sc.parallelize(["not safe",rd.count()])
rd = counts.filter(lambda x: "long waiting" in x)
rd = sc.parallelize(["long waiting",rd.count()])
result = result.union(rd)
rd = counts.filter(lambda x: "good school" in x)
rd = sc.parallelize(["good school",rd.count()])
result = result.union(rd)
rd = counts.filter(lambda x: "bad school" in x)
rd = sc.parallelize(["bad school",rd.count()])
result = result.union(rd)
rd = counts.filter(lambda x: "snow strom" in x)
rd = sc.parallelize(["snow strom",rd.count()])
result = result.union(rd)
result.saveAsTextFile("file:////home/ubuntu/data/keystringstaskd")